{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neural_Net(object):\n",
    "    '''\n",
    "    Defines network parameters\n",
    "    '''\n",
    "    def __init__(self, input_size, h1_size, h2_size, output_size = 1, lr = .025, hidden_layers = 2):\n",
    "\n",
    "        # Define network parameters\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.h1_size = h1_size\n",
    "        self.h2_size = h2_size\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Intialize weights\n",
    "        # scaled by .1 to be closer to 0\n",
    "        self.W1 = np.random.randn(self.input_size, self.h1_size) * .01\n",
    "        self.W2 = np.random.randn(self.h1_size, self.h2_size) * .01\n",
    "        self.W3 = np.random.randn(self.h2_size, self.output_size) * .01\n",
    "        \n",
    "        # Initialize biases\n",
    "        self.b1 = np.zeros(self.h1_size).reshape(1, -1)\n",
    "        self.b2 = np.zeros(self.h2_size).reshape(1, -1)\n",
    "        self.b3 = np.zeros(self.output_size).reshape(1, -1)\n",
    "        \n",
    "    '''\n",
    "    Performs a forward pass through the network.\n",
    "    This can be used to predict values.\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "\n",
    "        z1_dot = np.dot(x, self.W1) + self.b1 # x -> h1\n",
    "        self.z1 = self.ReLU(z1_dot) # f(x -> h1)\n",
    "        \n",
    "        z2_dot = np.dot(self.z1, self.W2) + self.b2 # h1 -> h2\n",
    "        self.z2 = self.ReLU(z2_dot) # f(h1 -> h2)\n",
    "        \n",
    "        z3_dot = np.dot(self.z2, self.W3) + self.b3 # h2 -> output\n",
    "        pred = self.ReLU(z3_dot) # f(h2 -> output)\n",
    "        \n",
    "        # return our output prediction\n",
    "        return pred\n",
    "    \n",
    "    '''\n",
    "    Our activation function\n",
    "    '''\n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    '''\n",
    "    Derivative of our activation function\n",
    "    '''\n",
    "    def dReLU(self, x):\n",
    "        # derivative of ReLU\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    '''\n",
    "    Returns the cost of a forward pass\n",
    "    '''\n",
    "    def get_cost(self, m, y, pred):\n",
    "        yhat = np.squeeze(pred)\n",
    "        Y = np.squeeze(y)\n",
    "        cost = np.sum((yhat - Y)**2)\n",
    "        cost = cost / m\n",
    "        return cost\n",
    "        \n",
    "    '''\n",
    "    Returns the derivative of our cost.\n",
    "    Use this for back-propagation.\n",
    "    '''\n",
    "    def get_cost_deriv(self, m, y, pred):\n",
    "        yhat = np.squeeze(pred)\n",
    "        Y = np.squeeze(y)\n",
    "        dCost = 2 * (yhat - Y) / m\n",
    "        return dCost\n",
    "\n",
    "    '''\n",
    "    Our backward pass through the network\n",
    "    ''' \n",
    "    def back_prop(self, x, y, pred):\n",
    "        \n",
    "        m = x.shape[0] # get length of input\n",
    "        \n",
    "        cost = self.get_cost(m, y, pred) # find the cost for predictions from hidden layer 2 -> output\n",
    "        dCost = self.get_cost_deriv(m, y, pred) # find the derivative of the cost\n",
    "        \n",
    "        pred_delta = (dCost * self.dReLU(np.squeeze(pred))) / m # find our first delta for h2 -> output\n",
    "        pred_delta = pred_delta.reshape(pred_delta.shape[0], self.W3.shape[1]) # reshape 0 dimension\n",
    "        \n",
    "        self.W3 -= self.lr * np.dot(self.z2.T, pred_delta) # update weight 3  \n",
    "        self.b3 -= self.lr * np.mean(pred_delta).T # update bias 3\n",
    "        \n",
    "        z2_err = np.dot(pred_delta, self.W3.T) # find cost for predictions between hidden layer 1 -> hidden layer 2\n",
    "        z2_delta = (z2_err * self.dReLU(self.z2)) / m # find delta for h1 -> h2\n",
    "        \n",
    "        self.W2 -= self.lr * np.dot(self.z1.T, z2_delta) # update weight 2\n",
    "        self.b2 -= self.lr * np.mean(z2_delta).T # update bias 2\n",
    "        \n",
    "        z1_err =  np.dot(z2_delta, self.W2.T) # find cost for predictions between input -> hidden layer 1\n",
    "        z1_delta = (z1_err * self.dReLU(self.z1)) / m # find delta for x -> h1\n",
    "        \n",
    "        self.W1 -= self.lr * np.dot(x.T, z1_delta) # update weight 1\n",
    "        self.b1 -= self.lr * np.mean(z1_delta).T # update bias 1\n",
    "\n",
    "        return cost # returns our cost so we can analyze it\n",
    "    \n",
    "    '''\n",
    "    Goes through a single training step\n",
    "    '''\n",
    "    def train_step(self, x, y, batch_size = 32):\n",
    "        avg_cost = 0.0 # create a cost\n",
    "\n",
    "        # cycle through training data in mini-batches\n",
    "        for i in range(0, x.shape[0], batch_size):\n",
    "            batch_train_x = x[i:i+batch_size]\n",
    "            batch_train_y = y[i:i+batch_size]\n",
    "            \n",
    "            # perform forward pass\n",
    "            pred = self.forward(batch_train_x)\n",
    "\n",
    "            # perform backward pass\n",
    "            # sum cost for each mini-batch pass\n",
    "            avg_cost += self.back_prop(batch_train_x, batch_train_y, pred)\n",
    "\n",
    "        avg_cost /= (x.shape[0] / batch_size) # divide sum by number of batches, this gives us avg cost of mini-batches per epoch\n",
    "        return avg_cost # return cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
